#!/bin/bash -e
# onetime - displays how long the current host was on each day

ELK_VERSION=8.14.1

# To generate a dump, run `ontime -k` and then:
# curl -XPOST -H "kbn-xsrf: reporting" localhost:5601/api/saved_objects/_export -H 'Content-Type: application/json' -d '{"type": ["index-pattern","dashboard"]}'
function kibana_dump() {
	cat <<'EOF'
{"attributes":{"description":"","kibanaSavedObjectMeta":{"searchSourceJSON":"{\"query\":{\"query\":\"\",\"language\":\"kuery\"},\"filter\":[]}"},"optionsJSON":"{\"useMargins\":true,\"syncColors\":false,\"syncCursor\":true,\"syncTooltips\":false,\"hidePanelTitles\":false}","panelsJSON":"[{\"type\":\"visualization\",\"gridData\":{\"x\":0,\"y\":0,\"w\":48,\"h\":28,\"i\":\"acb0b529-4fe2-4ad5-a2a7-c2bd629a4d74\"},\"panelIndex\":\"acb0b529-4fe2-4ad5-a2a7-c2bd629a4d74\",\"embeddableConfig\":{\"savedVis\":{\"id\":\"\",\"title\":\"\",\"description\":\"\",\"type\":\"vega\",\"params\":{\"spec\":\"{\\n  $schema: https://vega.github.io/schema/vega-lite/v5.json\\n  title: Duration by day (full days only, no outliers)\\n  data: {\\n    url: {\\n      index: ontime-stream\\n      body: {\\n        query: {\\n          bool: {\\n            must: [\\n              %dashboard_context-must_clause%\\n            ]\\n            must_not: [\\n              %dashboard_context-must_not_clause%\\n            ]\\n            filter: [\\n              %dashboard_context-filter_clause%\\n              {\\n                range: {\\n                  @timestamp: {\\n                    %timefilter%: true\\n                    shift: 10\\n                    unit: minute\\n                  }\\n                }\\n              }\\n            ]\\n          }\\n        }\\n        aggs: {\\n          time_buckets: {\\n            date_histogram: {\\n              field: @timestamp\\n              calendar_interval: 1d\\n              extended_bounds: {\\n                min: {\\n                  %timefilter%: min\\n                }\\n                max: {\\n                  %timefilter%: max\\n                }\\n              }\\n            }\\n            aggs: {\\n              sum_if_full_day: {\\n                scripted_metric: {\\n                  init_script: state.durations = []\\n                  map_script: state.durations.add(doc.duration.value)\\n                  combine_script: double sum = 0; for (d in state.durations) { if (d != null ) { sum += d } } return sum\\n                  reduce_script: double sum = 0; for (s in states) { if (s != null ) { sum += s } } return sum >= 6.0 && sum < 16.0 ? sum : Double.NaN\\n                }\\n              }\\n              mvng_avg: {\\n                moving_fn: {\\n                  buckets_path: sum_if_full_day.value\\n                  window: 10\\n                  script: MovingFunctions.unweightedAvg(values)\\n                  gap_policy: skip\\n                }\\n              }\\n            }\\n          }\\n        }\\n        size: 0\\n      }\\n    }\\n    format: {\\n      property: aggregations.time_buckets.buckets\\n    }\\n  }\\n  layer: [\\n    {\\n      mark: bar\\n      encoding: {\\n        x: {\\n          field: key\\n          type: temporal\\n          axis: {\\n            title: false\\n          }\\n        }\\n        y: {\\n          field: sum_if_full_day.value\\n          type: quantitative\\n          axis: {\\n            title: duration (hours)\\n          }\\n        }\\n      }\\n    }\\n    {\\n      mark: {\\n        stroke: \\\"#FF0000\\\"\\n        type: line\\n        interpolate: cardinal\\n      }\\n      encoding: {\\n        x: {\\n          field: key\\n          type: temporal\\n        }\\n        y: {\\n          field: mvng_avg.value\\n          type: quantitative\\n          axis: {\\n            title: duration (hours)\\n          }\\n        }\\n      }\\n    }\\n  ]\\n  resolve: {\\n    axis: {\\n      x: shared\\n      y: shared\\n    }\\n  }\\n}\"},\"uiState\":{},\"data\":{\"aggs\":[],\"searchSource\":{\"query\":{\"query\":\"\",\"language\":\"kuery\"},\"filter\":[]}}},\"enhancements\":{}}}]","timeRestore":false,"title":"Work time","version":2},"coreMigrationVersion":"8.8.0","created_at":"2024-07-05T09:44:39.424Z","id":"84a9d340-b393-11ed-9a0f-33fa85d316e5","managed":false,"references":[],"sort":[1720176902770,7],"type":"dashboard","typeMigrationVersion":"10.2.0","updated_at":"2024-07-05T10:55:02.770Z","version":"WzI5LDFd"}
{"attributes":{"fieldAttrs":"{}","fieldFormatMap":"{}","fields":"[]","name":"ontime-stream","runtimeFieldMap":"{}","sourceFilters":"[]","timeFieldName":"@timestamp","title":"ontime-stream","typeMeta":"{}"},"coreMigrationVersion":"8.8.0","created_at":"2024-07-05T09:44:39.424Z","id":"ontime-stream","managed":false,"references":[],"sort":[1720172679424,6],"type":"index-pattern","typeMigrationVersion":"8.0.0","updated_at":"2024-07-05T09:44:39.424Z","version":"WzYsMV0="}
{"excludedObjects":[],"excludedObjectsCount":0,"exportedCount":2,"missingRefCount":0,"missingReferences":[]}
EOF
}

function log() {
        echo 1>&2 "${@}"
}

function abort() {
        log "${@}"
        log 'Aborting.'
        exit 1
}

function success() {
        log "${@}"
        exit 0
}

function usage() {
        log -e "$(cat <<EOF
Usage:
	$0 [<options>]
        -c
                Display as CSV (default)
        -r
                Display raw data
        -k
                Explore data in Elasticsearch + Kibana, started in a container
EOF
)"
}

function format_raw() {
	cat
}

function format_csv() {
	awk '{print $6 "," $8}' | while IFS=, read -r start end
	do
		if [ "$end" = 'running' ]
		then
			end="$(date -Iseconds)"
		fi
		# https://stackoverflow.com/a/12723330/6692043
		# https://stackoverflow.com/a/8404392/6692043
		duration=$(bc -l <<< "x = ( $(date --date="$end" +%s) - $(date --date="$start" +%s) ) / 60.0 / 60.0; if(x<1) print 0; x")
		echo "$start,$end,$duration"
	done
}


function display_stdout() {
	sort -s -k 1 -t ','
}

function wait_success() {
	while ! curl 1>/dev/null -s "${@}" --fail
	do
		log "Not available yet; waiting..."
		sleep 2
	done
	log "Available."
}

function display_kibana() {
	COMPOSE=$(mktemp)
	cat >"$COMPOSE" <<EOF
version: '3.2'

services:
  elasticsearch:
    image: docker.io/elastic/elasticsearch:$ELK_VERSION
    environment:
      ES_JAVA_OPTS: "-Xms512m -Xmx512m"
      discovery.type: "single-node"
      xpack.security.enabled: "false"
      cluster.routing.allocation.disk.threshold_enabled: "false"
    networks:
      - elk
    ports:
      - "9200:9200"
  kibana:
    image: docker.io/elastic/kibana:$ELK_VERSION
    networks:
      - elk
    depends_on:
      - elasticsearch
    ports:
      - "5601:5601"

networks:
  elk:
    driver: bridge
EOF

	log "Compose file:"
	cat >&2 $COMPOSE
	
	trap "docker-compose -f $COMPOSE down -v ; rm $COMPOSE" EXIT
	docker-compose -f $COMPOSE up --detach

	log "Waiting for Elasticsearch to become available..."
	wait_success -H 'Content-Type: application/json' http://localhost:9200

	log "Indexing..."
	while IFS=, read -r start end duration
	do
		echo '{ "create" : { } }'
		echo "{\"@timestamp\": \"$start\", \"end\": \"$end\", \"duration\": $duration}"
	done | {
		i=0
		# https://stackoverflow.com/a/41268405/6692043
		while mapfile -n 50 lines && ((${#lines[@]}))
		do
			log "Indexing batch $(( i ++ ))..."
			{ echo "${lines[@]}"; echo; echo; } | curl 1>/dev/null -s -S -XPOST --fail-with-body -H 'Content-Type: application/json' --data-binary '@-' 'http:/localhost:9200/ontime-stream/_bulk'
		done
	}

	log "Waiting for Kibana to become available..."
	wait_success http://localhost:5601

	log "Waiting an extra few seconds for Kibana to be ready..."
	sleep 5

	log "Importing dashboard..."
	kibana_dump | curl 1>/dev/null -s -S -XPOST -H "kbn-xsrf: reporting" localhost:5601/api/saved_objects/_import --form "file=@-;filename=file.ndjson" --fail-with-body \
		|| log "Could not import dashboard; skipping."

	xdg-open http://localhost:5601/app/dashboards
	log "Kibana should now be opened in your browser."
	log "Press CTRL+C to stop..."
	read -r -d '' _ </dev/tty
}

format=csv
output=stdout

while getopts 'scrk' opt
do
        case "$opt" in
                c)
			format=csv
                        ;;
                r)
			format=raw
                        ;;
                k)
			format=csv
			output=kibana
                        ;;
                \?)
			usage
			abort
                        ;;
        esac
done

shift $(( OPTIND - 1 ))

last -x --time-format iso $(find /var/log/ -name 'wtmp*' 2>/dev/null | sed 's/^/-f /') runlevel | grep '^runlevel' \
	| format_$format | display_$output
